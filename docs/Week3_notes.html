<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Week3_notes</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Class Material</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Practical Machine Learning</a>
</li>
<li>
  <a href="https://cathydatascience.github.io/PracticalMachineLearning_Project/">Project Paper</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Notes
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Week1_notes.html">Week 1</a>
    </li>
    <li>
      <a href="Week2_notes.html">Week 2</a>
    </li>
    <li>
      <a href="Week3_notes.html">Week 3</a>
    </li>
    <li>
      <a href="Week4_notes.html">Week 4</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Quizzes
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Week2quiz_codes.html">Week 2</a>
    </li>
    <li class="divider"></li>
    <li>
      <a href="Week3quiz_codes.html">Week 3</a>
    </li>
    <li class="divider"></li>
    <li>
      <a href="Week4quiz_codes.html">Week 4</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="http://cathydatascience.github.io/showprojectsonline">
    <span class="fa fa-question fa-lg"></span>
     
    How to publish project paper(s) online
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Week3_notes</h1>

</div>


<div id="key-wordsconcepts-of-week-3" class="section level2">
<h2>Key Words/Concepts of Week 3</h2>
<p>This week focuses on the prediction of categorical variables.</p>
<ul>
<li>decision tree models<br />
</li>
<li>bagging
<ul>
<li>random forest model</li>
</ul></li>
<li>boosting
<ul>
<li>gradient boosting model</li>
</ul></li>
<li>parametric model (Bayesian update) prediction</li>
</ul>
</div>
<div id="decision-tree" class="section level2">
<h2>Decision Tree</h2>
<div id="key-ideas" class="section level3">
<h3>Key ideas</h3>
<ul>
<li>iteratively split variables into groups<br />
</li>
<li>evaluate “homogeneity” within each group<br />
</li>
<li>split again if necessary</li>
</ul>
<div id="features" class="section level4">
<h4><strong><em>features</em></strong></h4>
<ol style="list-style-type: decimal">
<li>classfication trees perform better in non-linear settings and easy to interpret</li>
<li>data transformation less important: monotonic transformation will not change outcome</li>
<li>results may vary and harder to estimate uncertainty</li>
<li>it may lead to overfitting without pruning/cross-validation</li>
</ol>
</div>
<div id="basic-algorithm" class="section level4">
<h4><strong><em>basic algorithm</em></strong></h4>
<ol style="list-style-type: decimal">
<li>Start with all variables in one group.</li>
<li>Find the variables/split that best separates the outcomes.</li>
<li>Divide the data into two groups (“leaves”) on that split (“node”).</li>
<li>Within each split, find the best variable/split that separates the outcomes (with <em>all</em> the variables)</li>
<li>Continue until the groups are too small or sufficiently “pure”.</li>
</ol>
</div>
<div id="measures-of-homogeneity-or-more-precise-impurity" class="section level4">
<h4><strong><em>measures of homogeneity (or more precise, impurity)</em></strong></h4>
<ol style="list-style-type: decimal">
<li>misclassification error (0 is perfect purity and 0.5 is no purity)</li>
<li>gini index (0 is perfect purity and 0.5 no purity)</li>
<li>deviance/information gain (or “entropy”) - 0 is perfect purity and 1 is no purity</li>
</ol>
</div>
</div>
<div id="example-of-decision-tree-model" class="section level3">
<h3>Example of Decision Tree Model</h3>
<pre class="r"><code>data(iris); library(ggplot2); library(caret)
names(iris)</code></pre>
<pre><code>## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot;  &quot;Petal.Length&quot; &quot;Petal.Width&quot; 
## [5] &quot;Species&quot;</code></pre>
<pre class="r"><code>table(iris$Species) #try to predict Species</code></pre>
<pre><code>## 
##     setosa versicolor  virginica 
##         50         50         50</code></pre>
<pre class="r"><code>inTrain&lt;-createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training&lt;-iris[inTrain, ]
testing&lt;-iris[-inTrain, ]
dim(training); dim(testing)</code></pre>
<pre><code>## [1] 105   5</code></pre>
<pre><code>## [1] 45  5</code></pre>
<pre class="r"><code># see clearly three groups which may be hard for linear model fitting
qplot(Petal.Width, Sepal.Width, colour=Species, data=training)</code></pre>
<p><img src="Week3_notes_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<pre class="r"><code>library(caret)
modFit&lt;-train(Species~., method=&quot;rpart&quot;, data=training) #r package in partition tree method
print(modFit$finalModel)</code></pre>
<pre><code>## n= 105 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
## 1) root 105 70 setosa (0.33333333 0.33333333 0.33333333)  
##   2) Petal.Length&lt; 2.45 35  0 setosa (1.00000000 0.00000000 0.00000000) *
##   3) Petal.Length&gt;=2.45 70 35 versicolor (0.00000000 0.50000000 0.50000000)  
##     6) Petal.Length&lt; 4.85 34  2 versicolor (0.00000000 0.94117647 0.05882353) *
##     7) Petal.Length&gt;=4.85 36  3 virginica (0.00000000 0.08333333 0.91666667) *</code></pre>
<pre class="r"><code>#an alterative way to see - graph
plot(modFit$finalModel, uniform=TRUE, main=&quot;Classification Tree&quot;) # dendogram
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=0.8)</code></pre>
<p><img src="Week3_notes_files/figure-html/unnamed-chunk-1-2.png" width="672" /></p>
<pre class="r"><code># prettier plots and need the &quot;rattle&quot; package
library(rattle)</code></pre>
<pre><code>## Rattle: A free graphical interface for data science with R.
## Version 5.2.0 Copyright (c) 2006-2018 Togaware Pty Ltd.
## Type &#39;rattle()&#39; to shake, rattle, and roll your data.</code></pre>
<pre class="r"><code>fancyRpartPlot(modFit$finalModel)</code></pre>
<p><img src="Week3_notes_files/figure-html/unnamed-chunk-1-3.png" width="672" /></p>
<pre class="r"><code># make prediction using the decision tree model
table(predict(modFit, newdata=testing), testing$Species)</code></pre>
<pre><code>##             
##              setosa versicolor virginica
##   setosa         15          0         0
##   versicolor      0         14         1
##   virginica       0          1        14</code></pre>
</div>
</div>
<div id="bagging-bootstrap-aggregating" class="section level2">
<h2>Bagging (bootstrap aggregating)</h2>
<div id="key-ideas-1" class="section level3">
<h3>Key ideas</h3>
<ol style="list-style-type: decimal">
<li>Resample observations and recalculate predictions with a type of model (which can be decision tree)</li>
<li>Use weighted average of models from the previous step or use majority vote for making predictions of an outcome</li>
</ol>
<p>The advantage of bagging is that it has similar bias as compared to a single model, but variance of the prediction is reduced. It has shown that bagging is more useful in non-linear setting.</p>
</div>
<div id="example-of-bagging" class="section level3">
<h3>Example of Bagging</h3>
<pre class="r"><code>library(ElemStatLearn); data(ozone, package=&quot;ElemStatLearn&quot;)</code></pre>
<pre><code>## 
## Attaching package: &#39;ElemStatLearn&#39;</code></pre>
<pre><code>## The following object is masked _by_ &#39;.GlobalEnv&#39;:
## 
##     spam</code></pre>
<pre class="r"><code>ozone&lt;-ozone[order(ozone$ozone), ] #try to predict temperature as a function of ozone
str(ozone)</code></pre>
<pre><code>## &#39;data.frame&#39;:    111 obs. of  4 variables:
##  $ ozone      : num  1 4 6 7 7 8 9 9 9 10 ...
##  $ radiation  : int  8 25 78 48 49 19 24 36 24 264 ...
##  $ temperature: int  59 61 57 80 69 61 81 72 71 73 ...
##  $ wind       : num  9.7 9.7 18.4 14.3 10.3 20.1 13.8 14.3 10.9 14.3 ...</code></pre>
<pre class="r"><code>#10 random samples with replacement from dataset 
#ozone variable ranges from 1 to 168 and we make prediction using 155 ozone numbers
ll&lt;-matrix(NA, nrow=10, ncol=155) 

for (i in 1:10){
    ss&lt;-sample(1:dim(ozone)[1], replace=T)
    ozone0&lt;-ozone[ss, ]; ozone0&lt;-ozone0[order(ozone0$ozone), ]
    #similar to spline, loess produces a smooth curve
    loess0&lt;-loess(temperature~ozone, data=ozone0, span=0.2) #span option controls how smooth the loess curve is 
    ll[i, ]&lt;-predict(loess0, newdata=data.frame(ozone=1:155)) 
}

#data points 
plot(ozone$ozone, ozone$temperature, pch=19, cex=0.5)
#the ten estimation lines from bootstrap samples
for (i in 1:10) {lines(1:155, ll[i, ], col=&quot;grey&quot;, lwd=2)} 
#the average estimation (bagging) line
lines(1:155, apply(ll, 2, mean), col=&quot;red&quot;, lwd=2)</code></pre>
<p><img src="Week3_notes_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>In the <code>train</code> function, other models are available for bagging. Use <code>method</code> options to choose <code>bagEarth</code>, <code>treebag</code>, and <code>bagFDA</code> etc..</p>
<p>Alternatively, you can build your own function using the <code>bag</code> function.</p>
<pre class="r"><code>library(party)
predictors=data.frame(ozone=ozone$ozone)
temperature=ozone$temperature
treebag&lt;-bag(predictors, temperature, B=10, bagControl=bagControl(fit=ctreeBag$fit, predict=ctreeBag$pred, aggregate=ctreeBag$aggregate))

#data points
plot(ozone$ozone, temperature, col=&quot;lightgrey&quot;, pch=19)
#first tree estimate
points(ozone$ozone, predict(treebag$fits[[1]]$fit, predictors), pch=19, col=&quot;red&quot;)
#combined 10 tree estimates
points(ozone$ozone, predict(treebag, predictors), pch=19, col=&quot;blue&quot;)</code></pre>
<p><img src="Week3_notes_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
<div id="random-forest" class="section level3">
<h3>Random Forest</h3>
<p>Bagging is often used with trees - an extension is random forests, which is highly accurate and popular in many data science contests.</p>
<div id="key-idea" class="section level4">
<h4>Key Idea</h4>
<ol style="list-style-type: decimal">
<li>Similar to bagging it uses bootstrap samples.</li>
<li>The difference is: at each split, the algorithm bootstraps <strong>only a subset of variables</strong>, which allows for a diverse number of trees.</li>
<li>From the diverse multiple trees, vote or use weighted average to get the final outcome</li>
</ol>
<p>The drawback is the method can be slow to run on computer, but you can use parallel computing. And, it may lead to overfitting, in which case, it is hard to tell which tree leads to the problem). It is a combination of many tree models, it can be harder to interpret the decision rule.</p>
</div>
<div id="example-of-random-forest" class="section level4">
<h4>Example of Random Forest</h4>
<pre class="r"><code>data(iris); library(ggplot2)
inTrain &lt;- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training &lt;- iris[inTrain,]
testing &lt;- iris[-inTrain,]

library(caret); library(randomForest)
modFit &lt;- train(Species~ .,data=training,method=&quot;rf&quot;,prox=TRUE) #&quot;rf&quot; is the random forest method; prox=TRUE produces extra info

getTree(modFit$finalModel,k=2) #look at a specific tree</code></pre>
<pre><code>##    left daughter right daughter split var split point status prediction
## 1              2              3         4        0.80      1          0
## 2              0              0         0        0.00     -1          1
## 3              4              5         3        4.85      1          0
## 4              6              7         1        4.95      1          0
## 5              8              9         4        1.75      1          0
## 6              0              0         0        0.00     -1          3
## 7              0              0         0        0.00     -1          2
## 8             10             11         4        1.60      1          0
## 9              0              0         0        0.00     -1          3
## 10             0              0         0        0.00     -1          3
## 11             0              0         0        0.00     -1          2</code></pre>
<pre class="r"><code>irisP &lt;- classCenter(training[,c(3,4)], training$Species, modFit$finalModel$prox) #take only `Petal.Length` and `Petal.Width` in irisP
irisP &lt;- as.data.frame(irisP); irisP$Species &lt;- rownames(irisP)
p &lt;- qplot(Petal.Width, Petal.Length, col=Species,data=training)
p + geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=5,shape=4,data=irisP)</code></pre>
<p><img src="Week3_notes_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>pred &lt;- predict(modFit,testing) 
testing$predRight &lt;- pred==testing$Species
table(pred,testing$Species)</code></pre>
<pre><code>##             
## pred         setosa versicolor virginica
##   setosa         15          0         0
##   versicolor      0         12         0
##   virginica       0          3        15</code></pre>
<pre class="r"><code>qplot(Petal.Width,Petal.Length,colour=predRight,data=testing,main=&quot;newdata Predictions&quot;)</code></pre>
<p><img src="Week3_notes_files/figure-html/unnamed-chunk-4-2.png" width="672" /></p>
</div>
</div>
</div>
<div id="boosting" class="section level2">
<h2>Boosting</h2>
<p>Along with random forest, boosting also produces very accurate prediction.</p>
<div id="key-idea-1" class="section level3">
<h3>Key idea</h3>
<ol style="list-style-type: decimal">
<li>Take lots of (possibly) weak models</li>
</ol>
<ul>
<li>take k classifiers typically from the same family (e.g. all possible trees, regression models, cutoff points)<br />
</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Weight the models to take advantage of their strengths and add them up</li>
<li>You end up with a stronger model</li>
</ol>
<p>The function is a weighted average of models.</p>
<p>The goal of minimized error on the training set.</p>
<p>The algorithm is to use iterative methods to select one classifier at each step, calculate the weights based on error, and upweight missed classifications and select next classifer.</p>
<p>One large subclass of boosting is gradient boosting. * <code>gbm</code> - boosting with trees * <code>mboost</code> - model based boosting * <code>ada</code> - statistical boosting based on additive logitstic regression</p>
<p>Adaboost is perhaps the most famous boosting method.<br />
<a href="http://en.wikipedia.org/wiki/AdaBoost">Adaboost on Wikipedia</a></p>
<p><a href="http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf">Helpful boosting tutorial</a></p>
</div>
<div id="example-of-boosting" class="section level3">
<h3>Example of Boosting</h3>
<pre class="r"><code>library(ISLR); data(Wage); library(ggplot2); library(caret); 
Wage&lt;-subset(Wage, select=-c(logwage))
inTrain&lt;-createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
training&lt;-Wage[inTrain, ]; testing&lt;-Wage[-inTrain, ]

#&quot;gbm&quot; is boosting with trees; verbose=FALSE gets ride of many intermediate output
modFit&lt;-train(wage~., method=&quot;gbm&quot;, data=training, verbose=FALSE)
#print(modFit) 

qplot(predict(modFit, testing), wage, data=testing)</code></pre>
<p><img src="Week3_notes_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</div>
</div>
<div id="parametric-model-bayesian-update-prediction" class="section level2">
<h2>Parametric Model (Bayesian update) Prediction</h2>
<div id="key-idea-2" class="section level3">
<h3>Key idea</h3>
<ol style="list-style-type: decimal">
<li><p>Assume the data follow a probalistic model.<br />
For example, if the outcome variable is count data, poisson distribution is appropriate.</p></li>
<li><p>Use Bayes’ theorem (probability of y in a particular class given the predictors taking on specific values equals to a fraction of the top and the bottom; the numerator being the probability of the features given y in the class times probablity of y in the class; the denominator is the probability of the features) to identify optimal classifiers based on the assumption of the model.</p></li>
</ol>
<p><span class="math display">\[Pr(Y=k|X=x) = \frac{Pr(X=x|Y=k)Pr(Y=k)}{\sum_{\ell=1}^K Pr(X=x|Y=\ell)Pr(Y=\ell)}\]</span></p>
<p>It is written as:<br />
<span class="math display">\[Pr(Y=k|X=x) = \frac{f_k(x) \pi_k}{\sum_{\ell = 1}^K f_{\ell}(x) \pi_{\ell}}\]</span> Let’s write out the multivariate model (many regressors):<br />
<span class="math display">\[P(Y = k | X_1,\ldots,X_m) = \frac{\pi_k P(X_1,\ldots,X_m| Y=k)}{\sum_{\ell = 1}^K P(X_1,\ldots,X_m | Y=\ell) \pi_{\ell}}\]</span></p>
<p>Prior probabilities (<span class="math inline">\(f_k(x)\)</span>) are specified in advance. A common choice is Gaussian (two parameters, mean and variance; for simplicity, assume off-diagonal covariance matrix elements are zero). Estimate the parameters from the data. Make prediction of y based on the calculated probabilities <span class="math inline">\(Pr(Y=k|X=x)\)</span> - category according to the highest probability.</p>
<p>Notice that for all observations, the denominator is the same. Hence, maximization of the numerator is equivalent to maximization of the probability.</p>
<p><span class="math display">\[ \propto \pi_k P(X_1,\ldots,X_m| Y=k)\]</span> We apply the Bayes theorem iteratively on each of the <span class="math inline">\(X_1, X_2..., X_m\)</span> and have the following: <span class="math display">\[P(X_1,\ldots,X_m, Y=k) = \pi_k P(X_1 | Y = k)P(X_2,\ldots,X_m | X_1,Y=k)\]</span> <span class="math display">\[ = \pi_k P(X_1 | Y = k) P(X_2 | X_1, Y=k) P(X_3,\ldots,X_m | X_1,X_2, Y=k)\]</span> <span class="math display">\[ = \pi_k P(X_1 | Y = k) P(X_2 | X_1, Y=k)\ldots P(X_m|X_1\ldots,X_{m-1},Y=k)\]</span></p>
<p>Several popular models:<br />
1. <strong>linear discriminant analysis</strong>: multivariate Gaussian with same covariances<br />
2. <strong>quadratic discriminant analysis</strong>: multivariate Gaussian with different covariances<br />
3. <strong>naive Bayes</strong>: assumes independent predictors (features are independent)</p>
<p>Naive Bayes is to assume independence across all Xs, which will allow us write out the probability terms as multiplcative of single feature given a classification.</p>
<p><span class="math display">\[ \approx \pi_k P(X_1 | Y = k) P(X_2 | Y = k)\ldots P(X_m |,Y=k)\]</span></p>
</div>
</div>

<p>Copyright &copy; 2019 Cathy Gao at cathygao.2019@outlook.com.</p>



</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
